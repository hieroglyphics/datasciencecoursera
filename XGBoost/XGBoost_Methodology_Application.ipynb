{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Methodology and Application\n",
    "\n",
    "*First Version Date: 2020-11-08   \n",
    "Latest update on: 2020-11-15*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. XGBoost vs. Gradient Boosting Machine (GBM)\n",
    "The full name for XGBoost is \"Extreme Gradient Boosting which is a specific implementation of the Gradient Boosting method using accurate approximations to find the best tree model. Specifically,\n",
    "\n",
    "1. XGBoost takes the *Taylor expansion* of the loss function up to the second order (i.e. second partial derivatives of the loss function). It simplifies the cost function and thus easy to compute.\n",
    "2. XGBoost has advanced regularization (L1 & L2), which improves model generalization.\n",
    "3. Training using XGBoost is very fast and can be parallelized / distributed across clusters.\n",
    "\n",
    "Quote from XGBoost Author Tianqi Chen:\n",
    "\n",
    "> *Both xgboost and gbm follow the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.*\n",
    "\n",
    "> *The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost. For model, it might be more suitable to be called as regularized gradient boosting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mathematical setup for Supervising Learning\n",
    "\n",
    "1. **Model**  \n",
    "A mathmatical structure or mapping to generate a prediction or estimate for $y_i$ from input $x_i$, where $i$ is the $i^{th}$ observation. Several examples as follows:\n",
    "  + Linear Regression Model: $\\hat{y}_i = \\sum_j\\theta_j x_{ij}$, where $j$ means the $j^{th}$ coefficient and $i$ is the $i^{th}$ observation.\n",
    "  + Generalized Linear Model or GLM (Logistic Regression is one of them): $\\hat{y}_i = g^{-1}\\left(\\sum_j\\theta_j x_{ij}\\right)$, where $g$ is the link function. Specifically, for logistic regression $g\\left(\\mu \\right) = \\log \\left( \\frac{\\mu}{1-\\mu} \\right)$ and $g^{-1}\\left(\\mu \\right)  = \\frac{1}{1 + \\exp^{-\\mu}}$. For more information on GLM, please refer to notebook <mark> TBA </mark>.\n",
    "  \n",
    "2. **Parameters**  \n",
    "Parameters are the part in the **Model** that needs to be determined in order to perform the prediction. In the Linear Regression Model and GLM case, $\\theta$ is the set of parameters that needs to be \"learnt\" from the data. \n",
    "\n",
    "3. **Model Training**  \n",
    "The process to find the **best** parameters $\\theta$ that fit the training data $X$ and the target variable $y$.\n",
    "\n",
    "4. **Objective Function**  \n",
    "The next natural questions is: How do we find the **best** parameter? We need to define the **objective function** for the parameters to best fit for the training data. The generic term for the objective function is as follows:  \n",
    "$$\\text{obj}(\\theta) = L(\\theta) + \\Omega(\\theta)$$ \n",
    "How to interpret the above function? There are two components which are illustrated as follows:\n",
    " + **Training Loss**  \n",
    " $L\\left( \\cdot \\right) $ is the training loss function. It measures how good or accurate prediction does the model have for the training data. Several examples as follows:\n",
    "     * The most commonly used one is the *mean squared error(MSE)* given by: $L(\\theta) = \\sum_i (y_i-\\hat{y}_i)^2$.\n",
    "     * Another commonly used loss is the *logistic loss* given by:\n",
    "     $L(\\theta) = \\sum_i\\left[ y_i\\ln (1+e^{-\\hat{y}_i}) + (1-y_i)\\ln (1+e^{\\hat{y}_i})\\right]$.\n",
    " \n",
    " + **Regularization Term**   \n",
    " $\\Omega \\left( \\cdot \\right)$ is the regularization term. It is a control from having a too complicated model and helps to avoid overfitting. This is generated from the principle to have a *parsimonious model*.  Parsimonious Models are simple models with great explanatory predictive power. They explain data with a minimum number of parameters, or predictor variables. Generally speaking, we need to develop a model that is a balance between *bias* and *variance*. For more illustration on **bias and variance tradeoff**, please refer to the notebook <mark> TBA</mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ensemble Tree Method\n",
    "\n",
    "### 3.1 Decision Trees\n",
    "For more information on the decision trees refer to another notebook: *<mark>Tree_Based_Methods.ipynb</mark>*.  \n",
    "### 3.2 Classification and Regression trees (CART)\n",
    "In CART, a real score is associated with each of the leaves, which gives us richer interpretations that allows for a principled, unified approach to optimization.\n",
    "\n",
    "### 3.3 Ensemble Methods\n",
    "Normally a single CART estimation is not accurate enough. Practically, different CARTs are integrated to generate the final prediction, e.g sum of different tree predictions as follows,\n",
    "$$\\hat{y}_i = \\sum_{k=1}^K f_k(x_i), f_k \\in \\mathcal{F}$$\n",
    "where $K$ is the total number of trees, $f_k\\left( \\cdot \\right)$ is a function in the function space $\\mathcal{F}$ and $\\mathcal{F}$ is the set of all CARTs.  \n",
    "And the objection function is defined as follows:\n",
    "$$\\text{obj}(\\theta) = \\sum_i^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)$$\n",
    "\n",
    "### 3.4 Boosting and Additive Training\n",
    "Learning tree structure is much harder than traditional optimization problem where you can simply take the gradient. It is intractable to learn all the trees at once. Instead, we can use an additive strategy: fix what we have learned, and add one new tree at a time. Let's rewrite the prediction and objection step by step:  \n",
    "For prediction:\n",
    "$$\\begin{split}\n",
    "\\hat{y}_i^{(0)} &= 0\\\\\n",
    "\\hat{y}_i^{(1)} &= f_1(x_i) = \\hat{y}_i^{(0)} + f_1(x_i)\\\\\n",
    "\\hat{y}_i^{(2)} &= f_1(x_i) + f_2(x_i)= \\hat{y}_i^{(1)} + f_2(x_i)\\\\\n",
    "\\dots\\\\\n",
    "\\hat{y}_i^{(t)} &= \\sum_{k=1}^t f_k(x_i)= \\hat{y}_i^{(t-1)} + f_t(x_i)\n",
    "\\end{split}$$\n",
    "For Objection function at step $t$:\n",
    "$$\\begin{split}\n",
    "\\text{obj}^{(t)} &= \\sum_i^n l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t \\Omega(f_i)\\\\\n",
    "                         &= \\sum_i^n l \\left( y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)\\right) + \\Omega(f_t) + \\mathrm{Constant}\n",
    "\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What are the specifics for XGBoost?\n",
    "\n",
    "### 4.1 Taylor Expansion\n",
    "As you can imagine the if the loss function is simple (meaning only with first order and quadratic terms like *MSE*), the optimization of the objective function is easier to solve. This generates the idea in XGBoost to use *Taylor Expansion of the loss function up to the second order* at $\\hat{y}_i^{(t-1)}$:\n",
    "$$\\begin{split}\n",
    "\\text{obj}^{(t)} &= \\sum_i^n l \\left( y_i, \\hat{y}_i^{(t-1)} + f_t(x_i)\\right) + \\Omega(f_t) + \\mathrm{Constant}\\\\\n",
    "&= \\sum_{i=1}^n \\left[ l(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)\\right] + \\Omega(f_t) + \\mathrm{Constant}\n",
    "\\end{split}$$\n",
    "where $g_i$ and $h_i$ are the first order and second order derivatives of $l(\\cdot)$ at $\\hat{y}_i^{(t-1)}$, specifically:\n",
    "$$\\begin{split}\n",
    "g_i &= \\partial_{\\hat{y}_i^{(t-1)}} l\\left(y_i, \\hat{y}_i^{(t-1)}\\right)\\\\\n",
    "h_i &= \\partial_{\\hat{y}_i^{(t-1)}}^2 l\\left(\\hat{y}_i^{(t-1)}\\right)\n",
    "\\end{split}$$\n",
    "Don't forget the target of the optimization: we need to find the best $f_t(\\cdot)$ that minimize the $obj^{(t)}$ and thus we can even simplify the objective function by removing the constants:\n",
    "$$\\sum_{i=1}^n \\left[g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)\\right] + \\Omega(f_t)$$\n",
    "As you can see from the above equation, the advantage is the value of the objective function only depends on $g_i$ and $h_i$. **In this sense, XGBoost can support customized loss functions as long as the first and second order derivative forms are provided!**  \n",
    "\n",
    "### 4.2 Regularization\n",
    "What about the regularization term $\\Omega(f_t)$? XGBoost redefines the tree $f_t(x)$ as:\n",
    "$$f_t(x) = w_{q(x)}, w \\in R^T, q:R^d\\rightarrow \\{1,2,\\cdots,T\\} $$\n",
    "Here $w$ is the vector of scores on leaves, $q$ is a function assigning each data point to the corresponding leaf, and $T$ is the number of leaves.  \n",
    "XGBoost defines the complexity or the regularization term as follows according to the re-written tree definition:\n",
    "$$\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$$\n",
    "where $w_j$ is the score for the $j^{th}$ leaf in the tree.  \n",
    "**Compared with other packages treating regularization less carefully, XGBoost defines the regularization term formmaly which can rigorously reduce the overfitting problem!**\n",
    "\n",
    "### 4.3 The new objective function and solution! \n",
    "After re-formulating the tree model, we can write the objective value with the $t^{th}$ tree as:\n",
    "$$\\begin{split}\n",
    "\\text{obj}^{(t)} &\\approx \\sum_{i=1}^n \\left[g_i w_{q(x_i)} + \\frac{1}{2} h_i w_{q(x_i)}^2 \\right] + \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2\\\\\n",
    "&= \\sum^T_{j=1} \\left[\\left(\\sum_{i\\in I_j} g_i\\right) w_j + \\frac{1}{2} \\left(\\sum_{i\\in I_j} h_i + \\lambda \\right) w_j^2 \\right] + \\gamma T\n",
    "\\end{split}$$  \n",
    "where $I_j = \\{i|q(x_i)=j\\}$ is the set of indices of data points assigned to the $j^{th}$ leaf.  \n",
    "\n",
    "Notice that in the second line we have changed the index of the summation because all the data points on the same leaf get the same score.  \n",
    "\n",
    "We could further compress the expression by defining $G_j = \\sum_{i\\in I_j} g_i$ and $H_j = \\sum_{i\\in I_j} h_i$ as follows:  \n",
    "$$\n",
    "\\text{obj}^{(t)} = \\sum^T_{j=1} \\left[G_jw_j + \\frac{1}{2} \\left(H_j+\\lambda \\right) w_j^2 \\right] +\\gamma T\\$$\n",
    "\n",
    "In this equation, $w_j$ is independent with respect to each other, the form $G_jw_j+\\frac{1}{2}\\left(H_j+\\lambda\\right)w_j^2$ is quadratic and the best $w_j$ for a given structure $q(x)$ and the best objective reduction we can get is\n",
    "$$\n",
    "\\begin{split}w_j^\\ast &= -\\frac{G_j}{H_j+\\lambda}\\\\\n",
    "\\text{obj}^\\ast &= -\\frac{1}{2} \\sum_{j=1}^T \\frac{G_j^2}{H_j+\\lambda} + \\gamma T\n",
    "\\end{split}$$\n",
    "The last equation measures *how good* a tree structure $q(x)$ is.  \n",
    "**Note**:This section is quoted the same as the XGBoost online tutorial.\n",
    "### 4.4 Tree Pruning\n",
    "TBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using XGBoost by hand - a 10 data-point example\n",
    "Inspired by Statquest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyper Parameters and Tuning for XGBoost\n",
    "### 6.1 Types of Parameters\n",
    "* **General Parameters**\n",
    "Which booster (tree or linear model) we want to choose for boosting\n",
    "* **Booster Parameters**\n",
    "Parameters related to the booster you choose.\n",
    "* **Task Parameters**\n",
    "Parameters decide the learning scenario.\n",
    "* **Command line parameters**\n",
    "Parameters that relate to behavior of CLI version of XGBoost.\n",
    "\n",
    "The following sections will illustrate important and often used parameters under each of the type.\n",
    "### 6.2 General Parameters\n",
    "    \n",
    "| Parameters | Description | Possible values |\n",
    "| :---|:--- | :--- |\n",
    "| `booster`(default = `gbtree`) | which booster to use | `gbtree`, `gblinear`, `dart`|\n",
    "|`verbosity`(default = `1`)| verbosity of printing messages| `0`(silent), `1`(warning), `2`(info), `3`(debug)|\n",
    "|`nthread`(default to maximum number of threads available if not set)|number of parallel threads used to run XGBoost| - |\n",
    "\n",
    "\n",
    "### 6.3 Booster Parameters\n",
    "#### 6.3.1 Tree Booster Parameters\n",
    "| Parameters | Description | Possible values |\n",
    "| :--|:-- | :-- |\n",
    "|`eta`(default = 0.3, or `learning_rate`)|Step size shrinkage used in update to prevents overfitting.|range: $[0, 1]$|\n",
    "|`gamma`(default = 0, or `min_split_loss`)|Minimum loss reduction required to make a further partition on a leaf node of the tree.|range: $[0,\\infty)$|\n",
    "|`max_depth`(default=6)|Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.|range: $[0,\\infty)$|\n",
    "|`min_child_weight`(default = 1)|Minimum sum of instance weight (hessian) needed in a child.The larger `min_child_weight` is, the more conservative the algorithm will be|range: $[0,\\infty)$|\n",
    "|`max_delta_step`(default = 0)|Maximum delta step we allow each leaf output to be.|range: $[0,\\infty)$|\n",
    "|`subsample`(default = 1)|Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees.|range: $(0, 1]$|\n",
    "|`sampling_method`(default = `uniform`)|The method to use to sample the training instances.|`uniform`, `gradient_based`|\n",
    "|`lambda`(default = 1, `reg_lambda`)|L2 regularization term on weights. Increasing this value will make model more conservative.|range: $[0,\\infty)$|\n",
    "|`alpha`(default = 0, `reg_alpha`|L1 regularization term on weights. Increasing this value will make model more conservative.|-|\n",
    "|`scale_pos_weight`(default = 1)|Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: `sum(negative instances) / sum(positive instances)`.|-|\n",
    "\n",
    "#### 6.3.2 Linear Booster Parameters\n",
    "| Parameters | Description | Possible values |\n",
    "| :--|:-- | :-- |\n",
    "|`lambda`(default = 1, `reg_lambda`)|L2 regularization term on weights. Increasing this value will make model more conservative.|range: $[0,\\infty)$|\n",
    "|`alpha`(default = 0, `reg_alpha`|L1 regularization term on weights. Increasing this value will make model more conservative.|-|\n",
    "\n",
    "### 6.4 Learning Task Parameters\n",
    "| Parameters | Description | Possible values |\n",
    "| :--|:-- | :-- |\n",
    "|`objective`(default = `reg:squarederror`)|Learning objectives|`reg:squarederror`, `reg:logistic`, `binary:logistic`, `count:poisson`, `survival:cox`, `reg:gamma`|\n",
    "|`base_score`(default = 0.5)|The initial prediction score of all instances|-|\n",
    "|`eval_metric`(default according to objective)|Evaluation metrics for validation data|`rmse`, `mae`,`logloss`, `error`, `mlogloss`, `auc`, `poisson-nloglik`, `gamma-nloglik`, `cox-nloglik`|\n",
    "|`seed`(default = 0)|Random number seed||\n",
    "\n",
    "### 6.5 Command Line Parameters\n",
    "TBA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://www.shirin-glander.de/2018/11/ml_basics_gbm/#:~:text=Gradient%20Boosting%20Machines%20vs.,XGBoost&text=While%20regular%20gradient%20boosting%20uses,order%20derivative%20as%20an%20approximation.\n",
    "2. https://xgboost.readthedocs.io/en/latest/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
